{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.38178238]\n",
      " [0.9204022 ]\n",
      " [0.9261161 ]\n",
      " [0.99575734]\n",
      " [0.8698294 ]\n",
      " [0.992681  ]\n",
      " [0.99207103]\n",
      " [0.9996064 ]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a = sess.run([train, hypot, pred, accuracy], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AND Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.01097435]\n",
      " [0.04650003]\n",
      " [0.04645649]\n",
      " [0.17636144]\n",
      " [0.04917124]\n",
      " [0.1850451 ]\n",
      " [0.185193  ]\n",
      " [0.49948454]]\n",
      "예측 : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "정확도 : 0.875\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a = sess.run([train, hypot, pred, accuracy], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.7499976]\n",
      " [0.7499988]\n",
      " [0.7499988]\n",
      " [0.75     ]\n",
      " [0.7499988]\n",
      " [0.75     ]\n",
      " [0.75     ]\n",
      " [0.7500012]]\n",
      "예측 : [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 : 0.75\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a = sess.run([train, hypot, pred, accuracy], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]]\n",
    "\n",
    "y_data = [0, 1, 1, 1, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(C=100)\n",
    "clf.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "examples = [[0, 0, 0], [1, 1, 1], [0, 1, 0], [1, 1, 0]]\n",
    "examples_label = [0, 0, 1, 1]\n",
    "\n",
    "result = clf.predict(examples)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score = metrics.accuracy_score(examples_label, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 딥러닝을 이용한 XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.03965613]\n",
      " [0.9760865 ]\n",
      " [0.97857404]\n",
      " [0.98768437]\n",
      " [0.98907423]\n",
      " [0.9721948 ]\n",
      " [0.97757626]\n",
      " [0.07700375]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# 두번째 레이어\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a = sess.run([train, hypot2, pred, accuracy], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide & Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.00694913]\n",
      " [0.99737257]\n",
      " [0.99822307]\n",
      " [0.9968605 ]\n",
      " [0.9979978 ]\n",
      " [0.99661744]\n",
      " [0.9967288 ]\n",
      " [0.0084672 ]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 레이어의 수는 7개(deep), 입출력 연결의 수는 50개(wide)로 구현\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "W1 = tf.Variable(tf.random_normal([3, 50]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# 두번째 레이어\n",
    "W2 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias2\")\n",
    "hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "\n",
    "# 세번째 레이어\n",
    "W3 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias3\")\n",
    "hypot3 = tf.sigmoid(tf.matmul(hypot2, W3) + b3)\n",
    "\n",
    "# 네번째 레이어\n",
    "W4 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight4\")\n",
    "b4 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias4\")\n",
    "hypot4 = tf.sigmoid(tf.matmul(hypot3, W4) + b4)\n",
    "\n",
    "# 다섯번째 레이어\n",
    "W5 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight5\")\n",
    "b5 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias5\")\n",
    "hypot5 = tf.sigmoid(tf.matmul(hypot4, W5) + b5)\n",
    "\n",
    "# 여섯번째 레이어\n",
    "W6 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight6\")\n",
    "b6 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias6\")\n",
    "hypot6 = tf.sigmoid(tf.matmul(hypot5, W6) + b6)\n",
    "\n",
    "# 일곱번째 레이어\n",
    "W7 = tf.Variable(tf.random_normal([50, 1]), tf.float32, name=\"weight7\")\n",
    "b7 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias7\")\n",
    "hypot7 = tf.sigmoid(tf.matmul(hypot6, W7) + b7)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot7) + (1 - y) * tf.log(1 - hypot7))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot7 > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a = sess.run([train, hypot7, pred, accuracy], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.05136064]\n",
      " [0.98422444]\n",
      " [0.99072415]\n",
      " [0.9592606 ]\n",
      " [0.9851891 ]\n",
      " [0.9463061 ]\n",
      " [0.9712459 ]\n",
      " [0.1183683 ]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "    hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"layer1\", hypot1)\n",
    "\n",
    "# 두번째 레이어\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "    hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"layer2\", hypot2)\n",
    "\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha01\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a, summary = sess.run([train, hypot2, pred, accuracy, merged_summary], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate tf1\n",
    "# tensorboard --logdir=./log_dir2/alpha01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.69818836]\n",
      " [0.760061  ]\n",
      " [0.7337137 ]\n",
      " [0.7588835 ]\n",
      " [0.7969161 ]\n",
      " [0.8022227 ]\n",
      " [0.7259207 ]\n",
      " [0.71067005]]\n",
      "예측 : [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 : 0.75\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.01\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "    hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"layer1\", hypot1)\n",
    "\n",
    "# 두번째 레이어\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "    hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"layer2\", hypot2)\n",
    "\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha001\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a, summary = sess.run([train, hypot2, pred, accuracy, merged_summary], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate tf1\n",
    "# tensorboard --logdir=./log_dir2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELU : Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-84fb6f4620c0>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\User_\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\User_\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\User_\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\User_\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\User_\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\User_\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x00000169DBC6BEC8>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x00000169DBC6BC48>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x00000169D8A86548>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫번째 모델 구축 : 성능 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logit = tf.matmul(X, W) + b\n",
    "hypot = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.3).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(hypot, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 2.3304922394319005\n",
      "epoch: 2     cost: 0.9182744961435146\n",
      "epoch: 3     cost: 0.7409407567977905\n",
      "epoch: 4     cost: 0.6529836184328252\n",
      "epoch: 5     cost: 0.596414811611175\n",
      "epoch: 6     cost: 0.5562223126129666\n",
      "epoch: 7     cost: 0.5260337093743415\n",
      "epoch: 8     cost: 0.50215968885205\n",
      "epoch: 9     cost: 0.481886454874819\n",
      "epoch: 10     cost: 0.46553927654569793\n",
      "epoch: 11     cost: 0.4508509802276438\n",
      "epoch: 12     cost: 0.4381981586326256\n",
      "epoch: 13     cost: 0.4269090359319339\n",
      "epoch: 14     cost: 0.41778768674893807\n",
      "epoch: 15     cost: 0.40824419801885403\n",
      "epoch: 16     cost: 0.40077131509780883\n",
      "epoch: 17     cost: 0.39392690685662374\n",
      "epoch: 18     cost: 0.38686319204893965\n",
      "epoch: 19     cost: 0.3812721705436709\n",
      "epoch: 20     cost: 0.3750885376063258\n",
      "epoch: 21     cost: 0.37049070515415883\n",
      "epoch: 22     cost: 0.365841796587814\n",
      "epoch: 23     cost: 0.3609442779150876\n",
      "epoch: 24     cost: 0.3565803019566968\n",
      "epoch: 25     cost: 0.35305826262994255\n",
      "epoch: 26     cost: 0.349766897342422\n",
      "epoch: 27     cost: 0.34637710324742615\n",
      "epoch: 28     cost: 0.34282333086837397\n",
      "epoch: 29     cost: 0.33949812081727127\n",
      "epoch: 30     cost: 0.3370067017728632\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9093\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 레이어 3개 추가, 입출력 갯수는 256개 : Relu 사용 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 2.9180853583595985\n",
      "epoch: 2     cost: 1.38771223415028\n",
      "epoch: 3     cost: 1.10290676615455\n",
      "epoch: 4     cost: 1.0108464765548708\n",
      "epoch: 5     cost: 0.8624337484619831\n",
      "epoch: 6     cost: 0.7785449233922088\n",
      "epoch: 7     cost: 0.7843555194681343\n",
      "epoch: 8     cost: 0.6861494182456626\n",
      "epoch: 9     cost: 0.6477327661080791\n",
      "epoch: 10     cost: 0.6364227480238136\n",
      "epoch: 11     cost: 0.6363442300666465\n",
      "epoch: 12     cost: 0.5887356165322389\n",
      "epoch: 13     cost: 0.5698062329942528\n",
      "epoch: 14     cost: 0.5854338043386279\n",
      "epoch: 15     cost: 0.5344946853681044\n",
      "epoch: 16     cost: 0.5402766026150095\n",
      "epoch: 17     cost: 0.5321127514405688\n",
      "epoch: 18     cost: 0.5155555638399992\n",
      "epoch: 19     cost: 0.5338102233409882\n",
      "epoch: 20     cost: 0.4879003593054685\n",
      "epoch: 21     cost: 0.4938588511943818\n",
      "epoch: 22     cost: 0.48910874009132377\n",
      "epoch: 23     cost: 0.4963414920460095\n",
      "epoch: 24     cost: 0.4629888438094745\n",
      "epoch: 25     cost: 0.44477155327796913\n",
      "epoch: 26     cost: 0.45969341635704064\n",
      "epoch: 27     cost: 0.45493383548476507\n",
      "epoch: 28     cost: 0.4391925350644373\n",
      "epoch: 29     cost: 0.4418127342787656\n",
      "epoch: 30     cost: 0.45033497951247475\n",
      "훈련 종료\n",
      "정확도 :  0.8719\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.sigmoid(logit3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.softmax(logit4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit4, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot4, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xavier 초기화 : 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "epoch: 1     cost: 1.0736246473138984\n",
      "epoch: 2     cost: 0.38994260370731343\n",
      "epoch: 3     cost: 0.3134769046306611\n",
      "epoch: 4     cost: 0.273540984229608\n",
      "epoch: 5     cost: 0.24362012538042951\n",
      "epoch: 6     cost: 0.21984309234402385\n",
      "epoch: 7     cost: 0.19854285194115198\n",
      "epoch: 8     cost: 0.18063280446962876\n",
      "epoch: 9     cost: 0.16385959519581358\n",
      "epoch: 10     cost: 0.15007857720960271\n",
      "epoch: 11     cost: 0.13837792358615184\n",
      "epoch: 12     cost: 0.12771100376139988\n",
      "epoch: 13     cost: 0.11852935598655183\n",
      "epoch: 14     cost: 0.110020270659165\n",
      "epoch: 15     cost: 0.10278451583602206\n",
      "epoch: 16     cost: 0.09598064085976644\n",
      "epoch: 17     cost: 0.0890547645295209\n",
      "epoch: 18     cost: 0.0833298005095937\n",
      "epoch: 19     cost: 0.07897893732921651\n",
      "epoch: 20     cost: 0.07383231316100466\n",
      "epoch: 21     cost: 0.06950145359743723\n",
      "epoch: 22     cost: 0.06507167811420828\n",
      "epoch: 23     cost: 0.061824095858768975\n",
      "epoch: 24     cost: 0.05819165544753726\n",
      "epoch: 25     cost: 0.054909798902544125\n",
      "epoch: 26     cost: 0.051550146283751215\n",
      "epoch: 27     cost: 0.04884298210794277\n",
      "epoch: 28     cost: 0.0459063890136101\n",
      "epoch: 29     cost: 0.043664613047784025\n",
      "epoch: 30     cost: 0.0408914291282946\n",
      "훈련 종료\n",
      "정확도 :  0.9753\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.sigmoid(logit3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.softmax(logit4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit4, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot4, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 좀 더 deep하고 wide하게 : layer 총 8개로 구성, 입출력 갯수는 512 : 97.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 2.004401129375804\n",
      "epoch: 2     cost: 0.5191619817235255\n",
      "epoch: 3     cost: 0.30098093487999655\n",
      "epoch: 4     cost: 0.21916998941789975\n",
      "epoch: 5     cost: 0.17377716766162352\n",
      "epoch: 6     cost: 0.14199405028061426\n",
      "epoch: 7     cost: 0.12039005948738621\n",
      "epoch: 8     cost: 0.10860162059014494\n",
      "epoch: 9     cost: 0.235669085112485\n",
      "epoch: 10     cost: 0.08405406545508992\n",
      "epoch: 11     cost: 0.07297314523295922\n",
      "epoch: 12     cost: 0.06259664962914854\n",
      "epoch: 13     cost: 0.053514020249924875\n",
      "epoch: 14     cost: 0.04894570135765456\n",
      "epoch: 15     cost: 0.04137351371347903\n",
      "epoch: 16     cost: 0.4268579447150908\n",
      "epoch: 17     cost: 0.0772134015404365\n",
      "epoch: 18     cost: 0.07440577312965288\n",
      "epoch: 19     cost: 0.03857976318083023\n",
      "epoch: 20     cost: 0.029557573893530788\n",
      "epoch: 21     cost: 0.02530288081700829\n",
      "epoch: 22     cost: 0.019618139641190104\n",
      "epoch: 23     cost: 0.016845055556940762\n",
      "epoch: 24     cost: 0.013590841083364053\n",
      "epoch: 25     cost: 0.01101568900143981\n",
      "epoch: 26     cost: 0.010464551613602612\n",
      "epoch: 27     cost: 0.00865263478893956\n",
      "epoch: 28     cost: 0.005932406373533678\n",
      "epoch: 29     cost: 0.25791921422113034\n",
      "epoch: 30     cost: 0.026354192969473948\n",
      "훈련 종료\n",
      "정확도 :  0.9773\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.relu(logit3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.relu(logit4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(hypot4, W5) + b5\n",
    "hypot5 = tf.nn.relu(logit5)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(hypot5, W6) + b6\n",
    "hypot6 = tf.nn.relu(logit6)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit7 = tf.matmul(hypot6, W7) + b7\n",
    "hypot7 = tf.nn.relu(logit7)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit8 = tf.matmul(hypot7, W8) + b8\n",
    "hypot8 = tf.nn.softmax(logit8)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit8, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot8, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropout : 과적합 해결방안 : 97.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-38-2118111333d8>:13: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "epoch: 1     cost: 2.370827011628586\n",
      "epoch: 2     cost: 1.7077324498783462\n",
      "epoch: 3     cost: 0.9269319846413357\n",
      "epoch: 4     cost: 0.5537715494632722\n",
      "epoch: 5     cost: 0.41826182706789544\n",
      "epoch: 6     cost: 0.347640887553042\n",
      "epoch: 7     cost: 0.2903929915753277\n",
      "epoch: 8     cost: 0.2545070690187543\n",
      "epoch: 9     cost: 0.22909609705209744\n",
      "epoch: 10     cost: 0.20840870889750382\n",
      "epoch: 11     cost: 0.19261213202368124\n",
      "epoch: 12     cost: 0.17918304199522192\n",
      "epoch: 13     cost: 0.16627841906114071\n",
      "epoch: 14     cost: 0.15617682748220188\n",
      "epoch: 15     cost: 0.1489064178954471\n",
      "epoch: 16     cost: 0.13875724762678152\n",
      "epoch: 17     cost: 0.13412081546404153\n",
      "epoch: 18     cost: 0.12634679841724317\n",
      "epoch: 19     cost: 0.12022890246049922\n",
      "epoch: 20     cost: 0.11440887643532317\n",
      "epoch: 21     cost: 0.11084614752368496\n",
      "epoch: 22     cost: 0.10485937307504094\n",
      "epoch: 23     cost: 0.1031146344948899\n",
      "epoch: 24     cost: 0.09858277486129238\n",
      "epoch: 25     cost: 0.09700229192321956\n",
      "epoch: 26     cost: 0.09107525935904541\n",
      "epoch: 27     cost: 0.08920794210312047\n",
      "epoch: 28     cost: 0.08486661645499145\n",
      "epoch: 29     cost: 0.08234926171431485\n",
      "epoch: 30     cost: 0.08308859847486018\n",
      "훈련 종료\n",
      "정확도 :  0.9776\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "hypot1 = tf.nn.dropout(hypot1, keep_prob=prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "hypot2 = tf.nn.dropout(hypot2, keep_prob=prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.relu(logit3)\n",
    "hypot3 = tf.nn.dropout(hypot3, keep_prob=prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.relu(logit4)\n",
    "hypot4 = tf.nn.dropout(hypot4, keep_prob=prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(hypot4, W5) + b5\n",
    "hypot5 = tf.nn.relu(logit5)\n",
    "hypot5 = tf.nn.dropout(hypot5, keep_prob=prob)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(hypot5, W6) + b6\n",
    "hypot6 = tf.nn.relu(logit6)\n",
    "hypot6 = tf.nn.dropout(hypot6, keep_prob=prob)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit7 = tf.matmul(hypot6, W7) + b7\n",
    "hypot7 = tf.nn.relu(logit7)\n",
    "hypot7 = tf.nn.dropout(hypot7, keep_prob=prob)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit8 = tf.matmul(hypot7, W8) + b8\n",
    "hypot8 = tf.nn.softmax(logit8)\n",
    "hypot8 = tf.nn.dropout(hypot8, keep_prob=prob)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit8, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot8, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys,\n",
    "                                                  prob:0.7})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels,\n",
    "                                              prob:1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropout : 과적합 해결방안 + AdamOptimizer: 98.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 0.9468834024667743\n",
      "epoch: 2     cost: 0.24832149670882656\n",
      "epoch: 3     cost: 0.1814348828115247\n",
      "epoch: 4     cost: 0.14601111166856512\n",
      "epoch: 5     cost: 0.12592802878130566\n",
      "epoch: 6     cost: 0.11183950004252521\n",
      "epoch: 7     cost: 0.09968389411202885\n",
      "epoch: 8     cost: 0.0908063989505171\n",
      "epoch: 9     cost: 0.08692868940193545\n",
      "epoch: 10     cost: 0.0836153724603355\n",
      "epoch: 11     cost: 0.07243979361754925\n",
      "epoch: 12     cost: 0.07477189270779491\n",
      "epoch: 13     cost: 0.07047422539104105\n",
      "epoch: 14     cost: 0.06206963706253605\n",
      "epoch: 15     cost: 0.06030392528934912\n",
      "epoch: 16     cost: 0.06349959340285165\n",
      "epoch: 17     cost: 0.056316831391304716\n",
      "epoch: 18     cost: 0.056132824954322806\n",
      "epoch: 19     cost: 0.05414022350175816\n",
      "epoch: 20     cost: 0.05182549765333532\n",
      "epoch: 21     cost: 0.04679628679698163\n",
      "epoch: 22     cost: 0.05152959807183252\n",
      "epoch: 23     cost: 0.04786871360530233\n",
      "epoch: 24     cost: 0.04853353266011584\n",
      "epoch: 25     cost: 0.04848193641984837\n",
      "epoch: 26     cost: 0.04732842456972736\n",
      "epoch: 27     cost: 0.039702664909206965\n",
      "epoch: 28     cost: 0.04449209934777833\n",
      "epoch: 29     cost: 0.041094646660018365\n",
      "epoch: 30     cost: 0.04146448447453706\n",
      "훈련 종료\n",
      "정확도 :  0.9822\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "hypot1 = tf.nn.dropout(hypot1, keep_prob=prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "hypot2 = tf.nn.dropout(hypot2, keep_prob=prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.relu(logit3)\n",
    "hypot3 = tf.nn.dropout(hypot3, keep_prob=prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.relu(logit4)\n",
    "hypot4 = tf.nn.dropout(hypot4, keep_prob=prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(hypot4, W5) + b5\n",
    "hypot5 = tf.nn.relu(logit5)\n",
    "hypot5 = tf.nn.dropout(hypot5, keep_prob=prob)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(hypot5, W6) + b6\n",
    "hypot6 = tf.nn.relu(logit6)\n",
    "hypot6 = tf.nn.dropout(hypot6, keep_prob=prob)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit7 = tf.matmul(hypot6, W7) + b7\n",
    "hypot7 = tf.nn.relu(logit7)\n",
    "hypot7 = tf.nn.dropout(hypot7, keep_prob=prob)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit8 = tf.matmul(hypot7, W8) + b8\n",
    "hypot8 = tf.nn.softmax(logit8)\n",
    "hypot8 = tf.nn.dropout(hypot8, keep_prob=prob)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit8, labels=y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot8, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys,\n",
    "                                                  prob:0.7})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels,\n",
    "                                              prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
